<ins>**SSPHP App**</ins>


<ins>Installation of the app from scratch</ins>


Steps : 

1. Installing the app from scratch involves first removing (uninstalling) the previous version of the app if there was one. Do this using 'Manage Apps' option in the Splunk Web UI.

2. Now upload and install the tar.gz for the app package that you have built, again using the Web UI.

3. Certain lookup csv files need to exist and they must be created by manually running saved searches. There are dependencies between the searches o create the csv files, so the order to run the queries in should be as follows :

ssphp_create_azure_fields_csv
ssphp_create_azure_resource_cmdb_csv

ssphp_generate_lookup_1_ssphp_cis_control_names_csv
ssphp_generate_lookup_2_ssphp_cis_v8_controls.csv
ssphp_generate_lookup_3_ssphp_policy_to_cis_csv

ssphp_create_historical_findings_times_csv



ssphp_write_findings_summary__azure_findings




    3.1. ssphp_create_resource_service_map_csv creates ssphp_resource_service_map.csv
    3.2. ssphp_create_base_azure_resource_list_csv creates ssphp_azure_resource_list.csv

    3.3 ssphp_create_github_public_repo_list_csv creates ssphp_github_public_repo_list.csv
    3.4 ssphp_create_github_repo_service_map_csv creates ssphp_github_repo_service_map.csv

    3.5 ssphp_create_map_ip_to_azure_rg_csv creates ssphp_map_ip_to_azure_rg.csv
    3.6 ssphp_create_qualys_service_list_csv creates ssphp_qualys_service_list.csv

    3.7 You might also want to manually run the use case searches so that there in a uptodate version of the data in the summary index

4. Accelerate the SSPHP data model **in your app**


======================================================================================================================================================================

<ins>Points to Note regarding Installation</ins>

By default, all saved searches are configured to be 'disabled' in the code savedsearches.conf. This means that any new saved searches will need to be manually enabled in the webui, as will all saved searches when thge app is installed from scratch.


======================================================================================================================================================================

<ins>Building the Package to Install</ins>

Terminal Command :  &'C:\Program Files\Python310\python.exe' .\package.py .\SSPHP\SSPHP\ [--dev]

Note.... build then commit (because the build increments the version number and we want the committed version to have the correct number)




======================================================================================================================================================================

**NOTES BY IAN & ALEX REGARDING RE-FACTORING THE CMDB ELEMENTS OF THE APPS TO REMOVE THE CSV FILES AND REPLACE THEM WITH LOADJOBS**

- The problem with using CSV files is that the CMDB is very large and consequently the knowledge bundle is very large for replication down onto the indexers
- The alternative might be to run the same saved search that creates the csv files, but instead just extend the expiration and refer to that data set using a loadjobs commend wherever a inputlookup or a lookup is currently being used
- We have not gone ahead and implemented this (20203-07-05) because the app is being parked for the time being and we are moving on to build new stuff, and it would take a lot of work to do it now. maybe we will come back to it at a later date.
- Part of the difficulty is that where there are currently automatic lookups, inputlookups, and lookups, we would need to replce them with joins to the new data set. This may become an issue because of the 50,000 (we changed it to 100,00 but still potentially an issue) limit on join.

- In order to have something to refer back to if and when we need to go down this path again, we created a saved search to make the data set, and another search to consume it. The search create the loadjob is the same as any other saved search but without the outputlookup, BUT extend the life by setting the parameter dispatch.tty to a larger number of seconds. The consuming search would look something like this :

```
`azure_index` sourcetype="azure:security:finding"
    [| search `azure_index` sourcetype="azure:security:finding" earliest=-7d@d latest=now
    | stats max(SSPHP_RUN) as SSPHP_RUN
    |  eval earliest_time=SSPHP_RUN-600, latest_time=SSPHP_RUN+3000, search_text="SSPHP_RUN=".SSPHP_RUN." earliest=".earliest_time." latest=".latest_time
    | return $search_text]
| fields ssphp_resource_id
| rename ssphp_resource_id as ssphp_id
| join type=outer ssphp_id 
    [| loadjob savedsearch="<your splunk user name>:SSPHP_DEV:ssphp_mem_test"]
| table ssphp_*
| search ssphp_level_1_display_name="*" AND ssphp_level_2_display_name="*"
```







